{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Ogni modello/tokenizer può essere richiamato con la sua classe o tramite la autoclass, che automaticamente instanzia l'oggetto corrispondente al nome del modello:\n",
        "\n",
        "```\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = BertModel.from_pretrained(model_name)\n",
        "```\n",
        "\n",
        "Dalla [documentazione](https://huggingface.co/docs/transformers/index) potete vedere tutte le classi e i modelli disponibili. Per il caricamento di un modello via autoclass di cui conoscete il nome ma non l'esatta dicitura, cerca il modello su [huggingface](https://huggingface.co/), vai sulla scheda del modello con il nome per il caricamento sul codice python, con descrizione e codice di esempio (vedi la [scheda](https://huggingface.co/roberta-base) di RoBERTa il cui nome del modello da inserire nel codice è `roberta-base`)"
      ],
      "metadata": {
        "id": "CfRZSi8QSZVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
        "\n",
        "model_name = \"bert-base-uncased\"\n",
        "config = AutoConfig.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name, config=config)"
      ],
      "metadata": {
        "id": "vr1AWVIm758m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# come funziona il tokenizzatore di BERT\n",
        "\n",
        "s = 'The quick brown fox, jumps over: the lazy dog.'\n",
        "print(len(s))\n",
        "print(s)\n",
        "print(len(tokenizer.tokenize(s)))\n",
        "print(tokenizer.tokenize(s)) # è la lista delle parole tokenizzate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaJEz3CWRxzl",
        "outputId": "6196ca71-2e58-4439-b19c-ed3c044ea91a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "46\n",
            "The quick brown fox, jumps over: the lazy dog.\n",
            "12\n",
            "['the', 'quick', 'brown', 'fox', ',', 'jumps', 'over', ':', 'the', 'lazy', 'dog', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tk = tokenizer(s, add_special_tokens=True, return_tensors=\"pt\")\n",
        "# restituisce un dizionario con i seguenti campi\n",
        "# input_ids -> tokens numerici\n",
        "# token_type_ids -> se 0 è la prima frase, se è 1 è la  seconda [CLS]\n",
        "# sentence 1 [SEP] sentence 2\n",
        "# attention_mask -> 1 se c'è un token in posizione, si vede meglio sotto quando\n",
        "# passo una lista di frasi da processare\n",
        "\n",
        "print('--- tensors ---')\n",
        "\n",
        "tk = tokenizer(s, add_special_tokens=True, return_tensors=\"pt\")\n",
        "print(tk['input_ids'].shape)\n",
        "print(type(tk['input_ids']))\n",
        "print(tk['input_ids'])\n",
        "print('*********')\n",
        "print(tk['attention_mask'].shape)\n",
        "print(type(tk['attention_mask']))\n",
        "print(tk['attention_mask'])\n",
        "\n",
        "print('--- no tensors ---')\n",
        "\n",
        "tk = tokenizer(s, add_special_tokens=False)\n",
        "print(len(tk['input_ids']))\n",
        "print(type(tk['input_ids']))\n",
        "print(tk['input_ids'])\n",
        "print('*********')\n",
        "print(len(tk['attention_mask']))\n",
        "print(type(tk['attention_mask']))\n",
        "print(tk['attention_mask'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtlh3FrMFzGd",
        "outputId": "f0e99be7-678c-4c1e-bfff-cf7db1cb3ff8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- tensors ---\n",
            "torch.Size([1, 14])\n",
            "<class 'torch.Tensor'>\n",
            "tensor([[  101,  1996,  4248,  2829,  4419,  1010, 14523,  2058,  1024,  1996,\n",
            "         13971,  3899,  1012,   102]])\n",
            "*********\n",
            "torch.Size([1, 14])\n",
            "<class 'torch.Tensor'>\n",
            "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
            "--- no tensors ---\n",
            "12\n",
            "<class 'list'>\n",
            "[1996, 4248, 2829, 4419, 1010, 14523, 2058, 1024, 1996, 13971, 3899, 1012]\n",
            "*********\n",
            "12\n",
            "<class 'list'>\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = ['The quick brown fox',\n",
        "     'jumps over: t',\n",
        "     'The quick brown fox, jumps']\n",
        "\n",
        "tk_1 = tokenizer(s, add_special_tokens=True, return_tensors=\"pt\", padding=True)\n",
        "tk_2 = tokenizer(s, add_special_tokens=False, return_tensors=\"pt\", padding=True)\n",
        "print(tk_1['input_ids'].shape)\n",
        "print(type(tk_1['input_ids']))\n",
        "print(tk_1['input_ids'])\n",
        "print('----')\n",
        "print(tk_2['input_ids'].shape)\n",
        "print(type(tk_2['input_ids']))\n",
        "print(tk_2['input_ids'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsbcpkK3Giok",
        "outputId": "b9534845-818e-4b71-fd3c-0ea9a42f26cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 8])\n",
            "<class 'torch.Tensor'>\n",
            "tensor([[  101,  1996,  4248,  2829,  4419,   102,     0,     0],\n",
            "        [  101, 14523,  2058,  1024,  1056,   102,     0,     0],\n",
            "        [  101,  1996,  4248,  2829,  4419,  1010, 14523,   102]])\n",
            "----\n",
            "torch.Size([3, 6])\n",
            "<class 'torch.Tensor'>\n",
            "tensor([[ 1996,  4248,  2829,  4419,     0,     0],\n",
            "        [14523,  2058,  1024,  1056,     0,     0],\n",
            "        [ 1996,  4248,  2829,  4419,  1010, 14523]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(tk['input_ids'], attention_mask=tk['input_ids'])\n",
        "last_hidden_states = outputs.last_hidden_state\n",
        "print(last_hidden_states.shape)\n",
        "# (n, l, d)\n",
        "    # n batch size\n",
        "    # l max len (padding value)\n",
        "    # d emb size\n",
        "print(last_hidden_states)\n",
        "#print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9-JhOeQGng5",
        "outputId": "2b1bcc31-781f-493f-acb5-4fd1611d433b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 14, 768])\n",
            "tensor([[[-0.0758,  0.0983,  0.1892,  ..., -0.0758,  0.1653,  0.1204],\n",
            "         [-0.4688,  0.0442,  0.1436,  ...,  0.0519,  0.2204, -0.2437],\n",
            "         [-0.1859,  0.0711,  0.1784,  ..., -0.1782,  0.2911,  0.1189],\n",
            "         ...,\n",
            "         [-0.1180,  0.2406,  0.1644,  ..., -0.2078,  0.2466,  0.4746],\n",
            "         [-0.4778,  0.1236,  0.1518,  ..., -0.0455,  0.1276, -0.0518],\n",
            "         [-0.3996,  0.2032,  0.1841,  ..., -0.0505,  0.1733,  0.1133]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "6-lrRQB6GzXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "\n",
        "model = \"bert-base-uncased\"\n",
        "task = \"feature-extraction\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "\n",
        "\n",
        "text = 'The quick brown fox, jumps over: the lazy dog.'\n",
        "tokenized_text = tokenizer.tokenize(s)\n",
        "\n",
        "feature_extractor = pipeline(task = task, model=model, tokenizer=tokenizer)\n",
        "result = feature_extractor(s, return_tensors=True)\n",
        "# con pipeline di hugging face fa la stessa cosa, estrae i cls token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Jj3vHkdG3C8",
        "outputId": "e1414292-f762-4b76-fc35-3c698d31fedb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(result == last_hidden_states).all()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhUcTYy2HE2D",
        "outputId": "fd518f7f-c0d6-4f97-a1d5-a3a4581b164e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(True)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ilFTxfgl7Sj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import sklearn\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXBCeCV6Fryo",
        "outputId": "babc2f35-769c-476b-bf91-fadce36807fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juAPPaWFmS-y"
      },
      "outputs": [],
      "source": [
        "# Dataset\n",
        "# repository https://github.com/iresiragusa/NLP/tree/main\n",
        "# https://www.kaggle.com/datasets/yufengdev/bbc-fulltext-and-category?select=bbc-text.csv\n",
        "# scarichiamo il dataset e lo carichiamo su COLAB\n",
        "\n",
        "root = \"/content/gdrive/MyDrive/Colab Notebooks/torch/\"\n",
        "df = pd.read_csv(root+\"data/BBC-text/bbc-text.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBaXhs_hmXfD"
      },
      "outputs": [],
      "source": [
        "# il dataset è formato da 2225 sample contenenti aricoli della BBC\n",
        "# suddivisi in 5 categorie in base al loro topic\n",
        "\n",
        "print('n sample -> '+str(len(df)))\n",
        "labels = set(df['category'])\n",
        "print('categories -> '+str(labels)+'['+str(len(labels))+']')\n",
        "print(df['category'].value_counts())\n",
        "\n",
        "# associo ad ogni categoria un indice, così ho delle label numeriche\n",
        "labels_dict = {\n",
        "    'business': 0,\n",
        "    'politics': 1,\n",
        "    'tech': 2,\n",
        "    'sport': 3,\n",
        "    'entertainment': 4\n",
        "}\n",
        "\n",
        "df['labels'] = df.apply(lambda row: labels_dict[row.category], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPmyopURmgI2"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "(x_train, x_test, y_train, y_test) = train_test_split(df['text'], df['labels'], test_size=0.2, random_state=17)\n",
        "(x_train, x_val, y_train, y_val) = train_test_split(x_train, y_train, test_size=0.1, random_state=17)\n",
        "\n",
        "# sarebbe uno split 72, 8, 20 per avere lo stesso test dell'altra volta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6nwHqTEmuKx"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, x, y, stopwords):\n",
        "\n",
        "        # x e y sono series di pandas\n",
        "        tokens_litt = [nltk.word_tokenize(text, language='english')\n",
        "         for text in list(x)]\n",
        "        text_clean = []\n",
        "\n",
        "        if stopwords:\n",
        "            for sentence in tqdm(tokens_litt, desc='Tokenizing ... '):\n",
        "                text_clean.append(' '.join([w.lower() for w in sentence if\n",
        "                    not w.lower() in nltk.corpus.stopwords.words(\"english\")]))\n",
        "        else:\n",
        "            for sentence in tqdm(tokens_litt, desc='Tokenizing ... '):\n",
        "                text_clean.append(' '.join([w.lower() for w in sentence]))\n",
        "            # ogni token è separato dall'altro con uno spazio\n",
        "\n",
        "        self.texts = text_clean\n",
        "        self.labels = [torch.tensor(label) for label in y]\n",
        "\n",
        "    def classes(self):\n",
        "        return self.labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        batch_texts = self.texts[idx]\n",
        "        batch_labels = np.array(self.labels[idx])\n",
        "\n",
        "        return batch_texts, batch_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwK_BUUzlFJ-"
      },
      "outputs": [],
      "source": [
        "hyperparameters = {\n",
        "    \"epochs\": 5,\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"batch_size\": 64,\n",
        "    \"dropout\": 0.1,\n",
        "    \"stopwords\": False,\n",
        "    \"language_model\": \"bert-base-uncased\",\n",
        "    \"layers\": 1,\n",
        "    \"h_dim\": 768,\n",
        "    \"bilstm\": True,\n",
        "    \"patience\": 5,\n",
        "    \"min_delta\": 0.01\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnC3SzpfpWrZ"
      },
      "outputs": [],
      "source": [
        "#creo i dataset\n",
        "\n",
        "train_dataset = Dataset(x_train, y_train, hyperparameters[\"stopwords\"])\n",
        "val_dataset = Dataset(x_val, y_val, hyperparameters[\"stopwords\"])\n",
        "test_dataset = Dataset(x_test, y_test, hyperparameters[\"stopwords\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KdTmVtrnCp6"
      },
      "outputs": [],
      "source": [
        "# classe della rete\n",
        "\n",
        "class ClassifierDeep(nn.Module):\n",
        "\n",
        "    def __init__(self, labels, hdim, dropout):\n",
        "        super(ClassifierDeep, self).__init__()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hdim, hdim),\n",
        "            nn.BatchNorm1d(hdim),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hdim, labels),\n",
        "            )\n",
        "\n",
        "    def forward(self, input_texts):\n",
        "        return self.classifier(input_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "In8JtAI6u-FE"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, min_delta=0.0):\n",
        "\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta              # valore minimo di decrescita della loss di validazione all'epoca corrente\n",
        "                                                # per asserire che c'è un miglioramenti della loss\n",
        "        self.counter = 0                        # contatore delle epoche di pazienza\n",
        "        self.early_stop = False                 # flag di early stop\n",
        "        self.min_validation_loss = torch.inf    # valore corrente ottimo della loss di validazione\n",
        "\n",
        "    def __call__(self, validation_loss):\n",
        "        # chiamata in forma funzionale dell'oggetto di classe EarlySopping\n",
        "\n",
        "        if (validation_loss + self.min_delta) >= self.min_validation_loss:  # la loss di validazione non decresce\n",
        "            self.counter += 1                                               # incrementiamo il contatore delle epoche di pazienza\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "                print(\"Early stop!\")\n",
        "        else:                                               # c'è un miglioramento della loss:\n",
        "            self.min_validation_loss = validation_loss      # consideriamo la loss corrente\n",
        "                                                            # come nuova loss ottimale\n",
        "            self.counter = 0                                # e azzeriamo il contatore di pazienza\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HELuTg_mFiTQ"
      },
      "outputs": [],
      "source": [
        "def gen_embeddings(input_id_text, attention_mask, lm_model):\n",
        "    with torch.no_grad():\n",
        "        last_hidden_states = lm_model(input_id_text, attention_mask=attention_mask).last_hidden_state\n",
        "        last_hidden_states = last_hidden_states[:,0,:]\n",
        "    return last_hidden_states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5jS3O-VvHv_"
      },
      "outputs": [],
      "source": [
        "def train_loop(model, dataloader, tokenizer, lm_model, loss, optimizer, device):\n",
        "    model.train()\n",
        "\n",
        "    epoch_acc = 0\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for batch_texts, batch_labels in tqdm(dataloader, desc='training set'):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        tokens = tokenizer(list(batch_texts), add_special_tokens=True, return_tensors='pt', padding='max_length', max_length = 512, truncation=True)\n",
        "        input_id_texts = tokens['input_ids'].squeeze(1).to(device)\n",
        "        mask_texts = tokens['attention_mask'].squeeze(1).to(device)\n",
        "        batch_labels = batch_labels.to(device)\n",
        "        embeddings_texts = gen_embeddings(input_id_texts, mask_texts, lm_model)\n",
        "        output = model(embeddings_texts)\n",
        "\n",
        "        # la loss è una CrossEntropyLoss, al suo interno ha\n",
        "        # la logsoftmax + negative log likelihood loss\n",
        "        batch_loss = loss(output, batch_labels)\n",
        "        batch_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += batch_loss.item()\n",
        "\n",
        "        # per calcolare l'accuracy devo generare le predizioni\n",
        "        # applicando manualmente la logsoftmax\n",
        "        softmax = nn.LogSoftmax(dim=1)\n",
        "        epoch_acc += (softmax(output).argmax(dim=1) == batch_labels).sum().item()\n",
        "\n",
        "        batch_labels = batch_labels.detach().cpu()\n",
        "        embeddings_texts = embeddings_texts.detach().cpu()\n",
        "        output = output.detach().cpu()\n",
        "\n",
        "    return epoch_loss/len(dataloader), epoch_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atbTc9ex1Q7S"
      },
      "outputs": [],
      "source": [
        "def test_loop(model, dataloader, tokenizer, lm_model, loss, device):\n",
        "    model.eval()\n",
        "\n",
        "    epoch_acc = 0\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for batch_texts, batch_labels, in tqdm(dataloader, desc='dev set'):\n",
        "\n",
        "            tokens = tokenizer(list(batch_texts), add_special_tokens=True,\n",
        "        return_tensors='pt', padding='max_length', max_length = 512, truncation=True)\n",
        "            input_id_texts = tokens['input_ids'].squeeze(1).to(device)\n",
        "            mask_texts = tokens['attention_mask'].squeeze(1).to(device)\n",
        "            batch_labels = batch_labels.to(device)\n",
        "            embeddings_texts = gen_embeddings(input_id_texts, mask_texts, lm_model)\n",
        "            output = model(embeddings_texts)\n",
        "\n",
        "            # la loss è una CrossEntropyLoss, al suo interno ha\n",
        "            # la logsoftmax + negative log likelihood loss\n",
        "            batch_loss = loss(output, batch_labels)\n",
        "            epoch_loss += batch_loss.item()\n",
        "\n",
        "            # per calcolare l'accuracy devo generare le predizioni\n",
        "            # applicando manualmente la logsoftmax\n",
        "            softmax = nn.LogSoftmax(dim=1)\n",
        "            epoch_acc += (softmax(output).argmax(dim=1) == batch_labels).sum().item()\n",
        "\n",
        "            batch_labels = batch_labels.detach().cpu()\n",
        "            embeddings_texts = embeddings_texts.detach().cpu()\n",
        "            output = output.detach().cpu()\n",
        "\n",
        "    return epoch_loss/len(dataloader), epoch_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lK_cnWEHMavf"
      },
      "outputs": [],
      "source": [
        "def train_test(model, epochs, optimizer, device, train_data, test_data,\n",
        "               batch_size, model_name, train_loss_fn,\n",
        "               test_loss_fn=None,         # non necessariamente train e test loss devono differire\n",
        "               early_stopping=None,       # posso addstrare senza early stopping\n",
        "               val_data=None,             # e in questo caso non c'è validation set\n",
        "               scheduler=None):           # possibile scheduler per monitorare l'andamento di un iperparametro,\n",
        "                                          # tipicamente il learning rate\n",
        "\n",
        "    train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    val_dataloader = torch.utils.data.DataLoader(val_data, batch_size=batch_size)\n",
        "    test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "    # check sulle funzioni di loss\n",
        "    if test_loss_fn == None:\n",
        "        test_loss_fn = train_loss_fn\n",
        "\n",
        "    # liste dei valori di loss e accuracy epoca per epoca per il plot\n",
        "    train_loss = []\n",
        "    validation_loss = []\n",
        "    test_loss = []\n",
        "\n",
        "    train_acc = []\n",
        "    validation_acc = []\n",
        "    test_acc = []\n",
        "\n",
        "    config = AutoConfig.from_pretrained(model_name)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    lm_model = AutoModel.from_pretrained(model_name, config=config).to(device)\n",
        "\n",
        "    # Ciclo di addestramento con early stopping\n",
        "    for epoch in tqdm(range(1,epochs+1)):\n",
        "\n",
        "        epoch_train_loss, epoch_train_acc = train_loop(model, train_dataloader, tokenizer, lm_model, train_loss_fn, optimizer, device)\n",
        "        train_loss.append(epoch_train_loss)\n",
        "        train_acc.append(epoch_train_acc/len(train_data))\n",
        "\n",
        "        # validation se è presente la callback di early stopping\n",
        "        if early_stopping != None:\n",
        "                epoch_validate_loss, epoch_validate_acc = test_loop(model,\n",
        "                val_dataloader, tokenizer, lm_model, test_loss_fn, device)\n",
        "                validation_loss.append(epoch_validate_loss)\n",
        "                validation_acc.append(epoch_validate_acc/len(val_data))\n",
        "\n",
        "        # test\n",
        "        epoch_test_loss, epoch_test_acc,= test_loop(model, test_dataloader, tokenizer, lm_model, test_loss_fn, device)\n",
        "        test_loss.append(epoch_test_loss)\n",
        "        test_acc.append(epoch_test_acc/len(test_data))\n",
        "\n",
        "        val_loss_str = f'Validation loss: {epoch_validate_loss:6.4f} ' if early_stopping != None else ' '\n",
        "        val_acc_str = f'Validation accuracy: {(epoch_validate_acc/len(val_data)):6.4f} ' if early_stopping != None else ' '\n",
        "        print(f\"\\nTrain loss: {epoch_train_loss:6.4f} {val_loss_str} Test loss: {epoch_test_loss:6.4f}\")\n",
        "        print(f\"Train accuracy: {(epoch_train_acc/len(train_data)):6.4f} {val_acc_str}Test accuracy: {(epoch_test_acc/len(test_data)):6.4f}\")\n",
        "\n",
        "        # early stopping\n",
        "        if early_stopping != None:\n",
        "                early_stopping(epoch_validate_loss)\n",
        "                if early_stopping.early_stop:\n",
        "                    break\n",
        "\n",
        "    return train_loss, validation_loss, test_loss, train_acc, validation_acc, test_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWgCYjxPPD8i"
      },
      "outputs": [],
      "source": [
        "# Acquisiamo il device su cui effettueremo il training\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "model = ClassifierDeep(len(labels_dict), hyperparameters[\"h_dim\"], hyperparameters[\"dropout\"]).to(device)\n",
        "print(model)\n",
        "\n",
        "# Calcoliamo il numero totale dei parametri del modello\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Numbero totale dei parametri: {total_params}\")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=hyperparameters[\"learning_rate\"])\n",
        "\n",
        "# Creiamo la callback di early stopping da passare al nostro metodo di addestramento\n",
        "early_stopping = EarlyStopping(patience=hyperparameters['patience'], min_delta=hyperparameters['min_delta'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEdv8U-wAL3L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2950c670-327d-4a12-e53d-fbc1c94430c5",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/5 [00:00<?, ?it/s]\n",
            "training set:   0%|          | 0/26 [00:00<?, ?it/s]\u001b[A\n",
            "training set:   4%|▍         | 1/26 [00:01<00:47,  1.88s/it]\u001b[A\n",
            "training set:   8%|▊         | 2/26 [00:03<00:44,  1.84s/it]\u001b[A\n",
            "training set:  12%|█▏        | 3/26 [00:05<00:42,  1.83s/it]\u001b[A\n",
            "training set:  15%|█▌        | 4/26 [00:07<00:40,  1.82s/it]\u001b[A\n",
            "training set:  19%|█▉        | 5/26 [00:09<00:38,  1.84s/it]\u001b[A\n",
            "training set:  23%|██▎       | 6/26 [00:11<00:37,  1.86s/it]\u001b[A\n",
            "training set:  27%|██▋       | 7/26 [00:13<00:35,  1.88s/it]\u001b[A\n",
            "training set:  31%|███       | 8/26 [00:14<00:33,  1.87s/it]\u001b[A\n",
            "training set:  35%|███▍      | 9/26 [00:16<00:31,  1.86s/it]\u001b[A\n",
            "training set:  38%|███▊      | 10/26 [00:18<00:29,  1.85s/it]\u001b[A\n",
            "training set:  42%|████▏     | 11/26 [00:20<00:27,  1.85s/it]\u001b[A\n",
            "training set:  46%|████▌     | 12/26 [00:22<00:25,  1.84s/it]\u001b[A\n",
            "training set:  50%|█████     | 13/26 [00:24<00:24,  1.86s/it]\u001b[A\n",
            "training set:  54%|█████▍    | 14/26 [00:26<00:22,  1.89s/it]\u001b[A\n",
            "training set:  58%|█████▊    | 15/26 [00:27<00:20,  1.88s/it]\u001b[A\n",
            "training set:  62%|██████▏   | 16/26 [00:29<00:18,  1.87s/it]\u001b[A\n",
            "training set:  65%|██████▌   | 17/26 [00:31<00:16,  1.86s/it]\u001b[A\n",
            "training set:  69%|██████▉   | 18/26 [00:33<00:14,  1.85s/it]\u001b[A\n",
            "training set:  73%|███████▎  | 19/26 [00:35<00:12,  1.85s/it]\u001b[A\n",
            "training set:  77%|███████▋  | 20/26 [00:37<00:11,  1.88s/it]\u001b[A\n",
            "training set:  81%|████████  | 21/26 [00:39<00:09,  1.90s/it]\u001b[A\n",
            "training set:  85%|████████▍ | 22/26 [00:41<00:07,  1.88s/it]\u001b[A\n",
            "training set:  88%|████████▊ | 23/26 [00:42<00:05,  1.88s/it]\u001b[A\n",
            "training set:  92%|█████████▏| 24/26 [00:44<00:03,  1.87s/it]\u001b[A\n",
            "training set: 100%|██████████| 26/26 [00:46<00:00,  1.80s/it]\n",
            "\n",
            "dev set:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "dev set:  33%|███▎      | 1/3 [00:01<00:03,  1.85s/it]\u001b[A\n",
            "dev set:  67%|██████▋   | 2/3 [00:03<00:01,  1.90s/it]\u001b[A\n",
            "dev set: 100%|██████████| 3/3 [00:05<00:00,  1.75s/it]\n",
            "\n",
            "dev set:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
            "dev set:  14%|█▍        | 1/7 [00:01<00:11,  1.96s/it]\u001b[A\n",
            "dev set:  29%|██▊       | 2/7 [00:03<00:09,  1.92s/it]\u001b[A\n",
            "dev set:  43%|████▎     | 3/7 [00:05<00:07,  1.90s/it]\u001b[A\n",
            "dev set:  57%|█████▋    | 4/7 [00:07<00:05,  1.91s/it]\u001b[A\n",
            "dev set:  71%|███████▏  | 5/7 [00:09<00:03,  1.90s/it]\u001b[A\n",
            "dev set:  86%|████████▌ | 6/7 [00:11<00:01,  1.89s/it]\u001b[A\n",
            "dev set: 100%|██████████| 7/7 [00:13<00:00,  1.88s/it]\n",
            " 20%|██        | 1/5 [01:05<04:20, 65.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train loss: 0.0616 Validation loss: 0.0908  Test loss: 0.0737\n",
            "Train accuracy: 0.9850 Validation accuracy: 0.9775 Test accuracy: 0.9730\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "training set:   0%|          | 0/26 [00:00<?, ?it/s]\u001b[A\n",
            "training set:   4%|▍         | 1/26 [00:02<00:50,  2.00s/it]\u001b[A\n",
            "training set:   8%|▊         | 2/26 [00:03<00:46,  1.94s/it]\u001b[A\n",
            "training set:  12%|█▏        | 3/26 [00:05<00:44,  1.93s/it]\u001b[A\n",
            "training set:  15%|█▌        | 4/26 [00:07<00:42,  1.92s/it]\u001b[A\n",
            "training set:  19%|█▉        | 5/26 [00:09<00:40,  1.91s/it]\u001b[A\n",
            "training set:  23%|██▎       | 6/26 [00:11<00:38,  1.91s/it]\u001b[A\n",
            "training set:  27%|██▋       | 7/26 [00:13<00:36,  1.93s/it]\u001b[A\n",
            "training set:  31%|███       | 8/26 [00:15<00:35,  1.95s/it]\u001b[A\n",
            "training set:  35%|███▍      | 9/26 [00:17<00:32,  1.93s/it]\u001b[A\n",
            "training set:  38%|███▊      | 10/26 [00:19<00:30,  1.92s/it]\u001b[A\n",
            "training set:  42%|████▏     | 11/26 [00:21<00:28,  1.91s/it]\u001b[A\n",
            "training set:  46%|████▌     | 12/26 [00:23<00:26,  1.92s/it]\u001b[A\n",
            "training set:  50%|█████     | 13/26 [00:25<00:24,  1.92s/it]\u001b[A\n",
            "training set:  54%|█████▍    | 14/26 [00:26<00:23,  1.93s/it]\u001b[A\n",
            "training set:  58%|█████▊    | 15/26 [00:28<00:21,  1.95s/it]\u001b[A\n",
            "training set:  62%|██████▏   | 16/26 [00:30<00:19,  1.93s/it]\u001b[A\n",
            "training set:  65%|██████▌   | 17/26 [00:32<00:17,  1.92s/it]\u001b[A\n",
            "training set:  69%|██████▉   | 18/26 [00:34<00:15,  1.91s/it]\u001b[A\n",
            "training set:  73%|███████▎  | 19/26 [00:36<00:13,  1.91s/it]\u001b[A\n",
            "training set:  77%|███████▋  | 20/26 [00:38<00:11,  1.91s/it]\u001b[A\n",
            "training set:  81%|████████  | 21/26 [00:40<00:09,  1.93s/it]\u001b[A\n",
            "training set:  85%|████████▍ | 22/26 [00:42<00:07,  1.96s/it]\u001b[A\n",
            "training set:  88%|████████▊ | 23/26 [00:44<00:05,  1.95s/it]\u001b[A\n",
            "training set:  92%|█████████▏| 24/26 [00:46<00:03,  1.94s/it]\u001b[A\n",
            "training set: 100%|██████████| 26/26 [00:48<00:00,  1.86s/it]\n",
            "\n",
            "dev set:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "dev set:  33%|███▎      | 1/3 [00:01<00:03,  1.91s/it]\u001b[A\n",
            "dev set:  67%|██████▋   | 2/3 [00:03<00:01,  1.92s/it]\u001b[A\n",
            "dev set: 100%|██████████| 3/3 [00:05<00:00,  1.78s/it]\n",
            "\n",
            "dev set:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
            "dev set:  14%|█▍        | 1/7 [00:02<00:12,  2.03s/it]\u001b[A\n",
            "dev set:  29%|██▊       | 2/7 [00:03<00:09,  1.96s/it]\u001b[A\n",
            "dev set:  43%|████▎     | 3/7 [00:05<00:07,  1.94s/it]\u001b[A\n",
            "dev set:  57%|█████▋    | 4/7 [00:07<00:05,  1.93s/it]\u001b[A\n",
            "dev set:  71%|███████▏  | 5/7 [00:09<00:03,  1.93s/it]\u001b[A\n",
            "dev set:  86%|████████▌ | 6/7 [00:11<00:01,  1.92s/it]\u001b[A\n",
            "dev set: 100%|██████████| 7/7 [00:13<00:00,  1.92s/it]\n",
            " 40%|████      | 2/5 [02:12<03:18, 66.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train loss: 0.0326 Validation loss: 0.0843  Test loss: 0.0721\n",
            "Train accuracy: 0.9925 Validation accuracy: 0.9775 Test accuracy: 0.9820\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "training set:   0%|          | 0/26 [00:00<?, ?it/s]\u001b[A\n",
            "training set:   4%|▍         | 1/26 [00:02<00:50,  2.02s/it]\u001b[A\n",
            "training set:   8%|▊         | 2/26 [00:03<00:47,  1.96s/it]\u001b[A\n",
            "training set:  12%|█▏        | 3/26 [00:05<00:44,  1.94s/it]\u001b[A\n",
            "training set:  15%|█▌        | 4/26 [00:07<00:42,  1.93s/it]\u001b[A\n",
            "training set:  19%|█▉        | 5/26 [00:09<00:40,  1.93s/it]\u001b[A\n",
            "training set:  23%|██▎       | 6/26 [00:11<00:38,  1.93s/it]\u001b[A\n",
            "training set:  27%|██▋       | 7/26 [00:13<00:37,  1.95s/it]\u001b[A\n",
            "training set:  31%|███       | 8/26 [00:15<00:35,  1.97s/it]\u001b[A\n",
            "training set:  35%|███▍      | 9/26 [00:17<00:33,  1.95s/it]\u001b[A\n",
            "training set:  38%|███▊      | 10/26 [00:19<00:31,  1.95s/it]\u001b[A\n",
            "training set:  42%|████▏     | 11/26 [00:21<00:29,  1.94s/it]\u001b[A\n",
            "training set:  46%|████▌     | 12/26 [00:23<00:27,  1.94s/it]\u001b[A\n",
            "training set:  50%|█████     | 13/26 [00:25<00:25,  1.94s/it]\u001b[A\n",
            "training set:  54%|█████▍    | 14/26 [00:27<00:23,  1.95s/it]\u001b[A\n",
            "training set:  58%|█████▊    | 15/26 [00:29<00:21,  1.98s/it]\u001b[A\n",
            "training set:  62%|██████▏   | 16/26 [00:31<00:19,  1.96s/it]\u001b[A\n",
            "training set:  65%|██████▌   | 17/26 [00:33<00:17,  1.96s/it]\u001b[A\n",
            "training set:  69%|██████▉   | 18/26 [00:35<00:15,  1.96s/it]\u001b[A\n",
            "training set:  73%|███████▎  | 19/26 [00:37<00:13,  1.95s/it]\u001b[A\n",
            "training set:  77%|███████▋  | 20/26 [00:39<00:11,  1.95s/it]\u001b[A\n",
            "training set:  81%|████████  | 21/26 [00:41<00:09,  1.96s/it]\u001b[A\n",
            "training set:  85%|████████▍ | 22/26 [00:43<00:07,  1.98s/it]\u001b[A\n",
            "training set:  88%|████████▊ | 23/26 [00:44<00:05,  1.97s/it]\u001b[A\n",
            "training set:  92%|█████████▏| 24/26 [00:46<00:03,  1.96s/it]\u001b[A\n",
            "training set: 100%|██████████| 26/26 [00:48<00:00,  1.88s/it]\n",
            "\n",
            "dev set:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "dev set:  33%|███▎      | 1/3 [00:01<00:03,  1.94s/it]\u001b[A\n",
            "dev set:  67%|██████▋   | 2/3 [00:03<00:01,  1.94s/it]\u001b[A\n",
            "dev set: 100%|██████████| 3/3 [00:05<00:00,  1.81s/it]\n",
            "\n",
            "dev set:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
            "dev set:  14%|█▍        | 1/7 [00:02<00:12,  2.01s/it]\u001b[A\n",
            "dev set:  29%|██▊       | 2/7 [00:03<00:09,  1.97s/it]\u001b[A\n",
            "dev set:  43%|████▎     | 3/7 [00:05<00:07,  1.96s/it]\u001b[A\n",
            "dev set:  57%|█████▋    | 4/7 [00:07<00:05,  1.96s/it]\u001b[A\n",
            "dev set:  71%|███████▏  | 5/7 [00:09<00:03,  1.95s/it]\u001b[A\n",
            "dev set:  86%|████████▌ | 6/7 [00:11<00:01,  1.95s/it]\u001b[A\n",
            "dev set: 100%|██████████| 7/7 [00:13<00:00,  1.94s/it]\n",
            " 60%|██████    | 3/5 [03:20<02:14, 67.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train loss: 0.0190 Validation loss: 0.0852  Test loss: 0.0659\n",
            "Train accuracy: 0.9975 Validation accuracy: 0.9719 Test accuracy: 0.9798\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "training set:   0%|          | 0/26 [00:00<?, ?it/s]\u001b[A\n",
            "training set:   4%|▍         | 1/26 [00:02<00:50,  2.02s/it]\u001b[A\n",
            "training set:   8%|▊         | 2/26 [00:03<00:47,  1.97s/it]\u001b[A\n",
            "training set:  12%|█▏        | 3/26 [00:05<00:44,  1.95s/it]\u001b[A\n",
            "training set:  15%|█▌        | 4/26 [00:07<00:42,  1.95s/it]\u001b[A\n",
            "training set:  19%|█▉        | 5/26 [00:09<00:40,  1.95s/it]\u001b[A\n",
            "training set:  23%|██▎       | 6/26 [00:11<00:39,  1.95s/it]\u001b[A\n",
            "training set:  27%|██▋       | 7/26 [00:13<00:37,  1.98s/it]\u001b[A\n",
            "training set:  31%|███       | 8/26 [00:15<00:35,  2.00s/it]\u001b[A\n",
            "training set:  35%|███▍      | 9/26 [00:17<00:33,  1.98s/it]\u001b[A\n",
            "training set:  38%|███▊      | 10/26 [00:19<00:31,  1.98s/it]\u001b[A\n",
            "training set:  42%|████▏     | 11/26 [00:21<00:29,  1.97s/it]\u001b[A\n",
            "training set:  46%|████▌     | 12/26 [00:23<00:27,  1.97s/it]\u001b[A\n",
            "training set:  50%|█████     | 13/26 [00:25<00:25,  1.97s/it]\u001b[A\n",
            "training set:  54%|█████▍    | 14/26 [00:27<00:23,  1.99s/it]\u001b[A\n",
            "training set:  58%|█████▊    | 15/26 [00:29<00:22,  2.01s/it]\u001b[A\n",
            "training set:  62%|██████▏   | 16/26 [00:31<00:19,  1.99s/it]\u001b[A\n",
            "training set:  65%|██████▌   | 17/26 [00:33<00:17,  1.98s/it]\u001b[A\n",
            "training set:  69%|██████▉   | 18/26 [00:35<00:15,  1.97s/it]\u001b[A\n",
            "training set:  73%|███████▎  | 19/26 [00:37<00:13,  1.97s/it]\u001b[A\n",
            "training set:  77%|███████▋  | 20/26 [00:39<00:11,  1.97s/it]\u001b[A\n",
            "training set:  81%|████████  | 21/26 [00:41<00:09,  1.98s/it]\u001b[A\n",
            "training set:  85%|████████▍ | 22/26 [00:43<00:07,  2.00s/it]\u001b[A\n",
            "training set:  88%|████████▊ | 23/26 [00:45<00:05,  1.99s/it]\u001b[A\n",
            "training set:  92%|█████████▏| 24/26 [00:47<00:03,  1.98s/it]\u001b[A\n",
            "training set: 100%|██████████| 26/26 [00:49<00:00,  1.91s/it]\n",
            "\n",
            "dev set:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "dev set:  33%|███▎      | 1/3 [00:01<00:03,  1.95s/it]\u001b[A\n",
            "dev set:  67%|██████▋   | 2/3 [00:03<00:01,  1.96s/it]\u001b[A\n",
            "dev set: 100%|██████████| 3/3 [00:05<00:00,  1.81s/it]\n",
            "\n",
            "dev set:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
            "dev set:  14%|█▍        | 1/7 [00:02<00:12,  2.03s/it]\u001b[A\n",
            "dev set:  29%|██▊       | 2/7 [00:03<00:09,  1.99s/it]\u001b[A\n",
            "dev set:  43%|████▎     | 3/7 [00:05<00:07,  1.97s/it]\u001b[A\n",
            "dev set:  57%|█████▋    | 4/7 [00:07<00:05,  1.98s/it]\u001b[A\n",
            "dev set:  71%|███████▏  | 5/7 [00:09<00:03,  1.97s/it]\u001b[A\n",
            "dev set:  86%|████████▌ | 6/7 [00:11<00:01,  1.96s/it]\u001b[A\n",
            "dev set: 100%|██████████| 7/7 [00:13<00:00,  1.95s/it]\n",
            " 80%|████████  | 4/5 [04:28<01:07, 67.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train loss: 0.0101 Validation loss: 0.0994  Test loss: 0.0726\n",
            "Train accuracy: 1.0000 Validation accuracy: 0.9719 Test accuracy: 0.9798\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "training set:   0%|          | 0/26 [00:00<?, ?it/s]\u001b[A\n",
            "training set:   4%|▍         | 1/26 [00:02<00:50,  2.02s/it]\u001b[A\n",
            "training set:   8%|▊         | 2/26 [00:03<00:47,  1.99s/it]\u001b[A\n",
            "training set:  12%|█▏        | 3/26 [00:05<00:45,  1.98s/it]\u001b[A\n",
            "training set:  15%|█▌        | 4/26 [00:07<00:43,  1.98s/it]\u001b[A\n",
            "training set:  19%|█▉        | 5/26 [00:09<00:41,  1.98s/it]\u001b[A\n",
            "training set:  23%|██▎       | 6/26 [00:11<00:39,  1.98s/it]\u001b[A\n",
            "training set:  27%|██▋       | 7/26 [00:13<00:38,  2.01s/it]\u001b[A\n",
            "training set:  31%|███       | 8/26 [00:15<00:36,  2.01s/it]\u001b[A\n",
            "training set:  35%|███▍      | 9/26 [00:17<00:33,  2.00s/it]\u001b[A\n",
            "training set:  38%|███▊      | 10/26 [00:19<00:31,  1.99s/it]\u001b[A\n",
            "training set:  42%|████▏     | 11/26 [00:21<00:29,  1.98s/it]\u001b[A\n",
            "training set:  46%|████▌     | 12/26 [00:23<00:27,  1.98s/it]\u001b[A\n",
            "training set:  50%|█████     | 13/26 [00:25<00:25,  1.98s/it]\u001b[A\n",
            "training set:  54%|█████▍    | 14/26 [00:27<00:23,  2.00s/it]\u001b[A\n",
            "training set:  58%|█████▊    | 15/26 [00:29<00:22,  2.01s/it]\u001b[A\n",
            "training set:  62%|██████▏   | 16/26 [00:31<00:19,  2.00s/it]\u001b[A\n",
            "training set:  65%|██████▌   | 17/26 [00:33<00:17,  1.99s/it]\u001b[A\n",
            "training set:  69%|██████▉   | 18/26 [00:35<00:15,  1.99s/it]\u001b[A\n",
            "training set:  73%|███████▎  | 19/26 [00:37<00:13,  1.99s/it]\u001b[A\n",
            "training set:  77%|███████▋  | 20/26 [00:39<00:11,  1.98s/it]\u001b[A\n",
            "training set:  81%|████████  | 21/26 [00:41<00:10,  2.02s/it]\u001b[A\n",
            "training set:  85%|████████▍ | 22/26 [00:43<00:08,  2.01s/it]\u001b[A\n",
            "training set:  88%|████████▊ | 23/26 [00:45<00:06,  2.00s/it]\u001b[A\n",
            "training set:  92%|█████████▏| 24/26 [00:47<00:03,  1.99s/it]\u001b[A\n",
            "training set: 100%|██████████| 26/26 [00:49<00:00,  1.92s/it]\n",
            "\n",
            "dev set:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "dev set:  33%|███▎      | 1/3 [00:01<00:03,  1.98s/it]\u001b[A\n",
            "dev set:  67%|██████▋   | 2/3 [00:03<00:01,  1.99s/it]\u001b[A\n",
            "dev set: 100%|██████████| 3/3 [00:05<00:00,  1.83s/it]\n",
            "\n",
            "dev set:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
            "dev set:  14%|█▍        | 1/7 [00:02<00:12,  2.06s/it]\u001b[A\n",
            "dev set:  29%|██▊       | 2/7 [00:04<00:10,  2.01s/it]\u001b[A\n",
            "dev set:  43%|████▎     | 3/7 [00:06<00:07,  1.99s/it]\u001b[A\n",
            "dev set:  57%|█████▋    | 4/7 [00:08<00:05,  1.99s/it]\u001b[A\n",
            "dev set:  71%|███████▏  | 5/7 [00:09<00:03,  1.99s/it]\u001b[A\n",
            "dev set:  86%|████████▌ | 6/7 [00:11<00:01,  1.98s/it]\u001b[A\n",
            "dev set: 100%|██████████| 7/7 [00:13<00:00,  1.98s/it]\n",
            "100%|██████████| 5/5 [05:38<00:00, 67.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train loss: 0.0086 Validation loss: 0.0906  Test loss: 0.0676\n",
            "Train accuracy: 0.9994 Validation accuracy: 0.9719 Test accuracy: 0.9775\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Routine di addestramento\n",
        "train_loss, validation_loss,test_loss,\n",
        "train_acc, validation_acc, test_acc = train_test(model, hyperparameters['epochs'], optimizer, device, train_dataset,\n",
        "test_dataset, hyperparameters['batch_size'], hyperparameters['language_model'], criterion, criterion, early_stopping, val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize=(20, 10))\n",
        "\n",
        "axs[0].plot(train_loss, label='training loss')\n",
        "axs[0].plot(validation_loss, label='validation loss')\n",
        "axs[0].plot(test_loss, label='test loss')\n",
        "axs[0].legend(loc='upper right')\n",
        "axs[0].set_ylim(0,1)\n",
        "\n",
        "axs[1].plot(train_acc, label='training accuracy')\n",
        "axs[1].plot(validation_acc, label='validation accuracy')\n",
        "axs[1].plot(test_acc, label='test accuracy')\n",
        "axs[1].legend(loc='lower right')\n",
        "axs[1].set_ylim(0,1)"
      ],
      "metadata": {
        "id": "v3jFVtjAzFqO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}